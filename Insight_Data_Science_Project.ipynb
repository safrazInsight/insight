{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Subscribers Using Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import re, math\n",
    "import warnings\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from patsy import dmatrices\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions for Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv(csv_var_string):\n",
    "    return pd.read_csv(csv_var_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates used in the synthesized user-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(values):\n",
    "    output = []\n",
    "    for value in values:\n",
    "        if value not in output:\n",
    "            output.append(value)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorganize keyword feature phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def synthesize_keywords(df_col_arg):\n",
    "    converted_keywords = df_col_arg\n",
    "    converted_linear_keywords = []\n",
    "    for keyword in converted_keywords:\n",
    "        converted_linear_keywords.append(str(keyword).lower())    \n",
    "    for i in range(0,len(converted_linear_keywords)):\n",
    "        converted_linear_keywords[i] = \"\".join(c for c in converted_linear_keywords[i] if c not in punc)\n",
    "    converted_linear_keywords = [[feature for feature in clk.split() if feature not in stoplist] \n",
    "         for clk in converted_linear_keywords]\n",
    "    for i in range(0,len(converted_linear_keywords)):\n",
    "        converted_linear_keywords[i] = remove_duplicates(converted_linear_keywords[i])\n",
    "    return converted_linear_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorganize industry feature phrases, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def synthesize_industry(df_col_arg):\n",
    "    converted_industry = df_col_arg\n",
    "    converted_linear_industry = []\n",
    "    for industry in converted_industry:\n",
    "        converted_linear_industry.append(str(industry).lower())    \n",
    "    for i in range(0,len(converted_linear_industry)):\n",
    "        converted_linear_industry[i] = re.sub(\"[^a-zA-Z]\", \" \",converted_linear_industry[i] )\n",
    "    for i in range(0,len(converted_linear_industry)):\n",
    "        converted_linear_industry[i] = \"\".join(c for c in converted_linear_industry[i] if c not in punc)\n",
    "    converted_linear_industry = [[feature for feature in cli.split() if feature not in stoplist] \n",
    "         for cli in converted_linear_industry]\n",
    "    for i in range(0,len(converted_linear_industry)):\n",
    "        converted_linear_industry[i] = remove_duplicates(converted_linear_industry[i])\n",
    "    return converted_linear_industry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorganize seo feature phrases, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def synthesize_seo(df_col_arg):\n",
    "    converted_seo = df_col_arg\n",
    "    converted_linear_seo = []\n",
    "    for seo in converted_seo:\n",
    "        converted_linear_seo.append(str(seo).lower())    \n",
    "    for i in range(0,len(converted_linear_seo)):\n",
    "        converted_linear_seo[i] = re.sub(\"[^a-zA-Z]\", \" \",converted_linear_seo[i] )  # The text to search\n",
    "        converted_linear_seo[i] = re.sub('null', '', converted_linear_seo[i])\n",
    "        converted_linear_seo[i] = re.sub('{{pagetitledescription}}', '', converted_linear_seo[i])\n",
    "        converted_linear_seo[i] = re.sub('homepage\\xce\\xbe', '', converted_linear_seo[i])\n",
    "        converted_linear_seo[i] = re.sub('world\\xce\\xbe', 'world', converted_linear_seo[i])\n",
    "        converted_linear_seo[i] = re.sub('worklife', 'worklife', converted_linear_seo[i])\n",
    "        converted_linear_seo[i] = re.sub('wwwwepowcom', '', converted_linear_seo[i])\n",
    "        converted_linear_seo[i] = re.sub('s_', '', converted_linear_seo[i])\n",
    "        converted_linear_seo[i] = re.sub('metadescription', '', converted_linear_seo[i])\n",
    "        converted_linear_seo[i] = re.sub('\\xce\\xbelemonlight', '', converted_linear_seo[i])\n",
    "        converted_linear_seo[i] = re.sub('\\xce', '', converted_linear_seo[i])\n",
    "        converted_linear_seo[i] = re.sub('\\xbe', '', converted_linear_seo[i])    \n",
    "    for i in range(0,len(converted_linear_seo)):\n",
    "        converted_linear_seo[i] = \"\".join(c for c in converted_linear_seo[i] if c not in punc)\n",
    "    converted_linear_seo = [[feature for feature in cls.split() if feature not in stoplist] \n",
    "         for cls in converted_linear_seo]\n",
    "    for i in range(0,len(converted_linear_seo)):\n",
    "        converted_linear_seo[i] = remove_duplicates(converted_linear_seo[i])\n",
    "    return converted_linear_seo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorganize technologies feature phrases, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def synthesize_technologies(df_col_arg):\n",
    "    converted_technologies = df_col_arg\n",
    "    converted_linear_technologies = []\n",
    "    for tech in converted_technologies:\n",
    "        converted_linear_technologies.append(str(tech).lower())    \n",
    "    for i in range(0,len(converted_linear_technologies)):\n",
    "        converted_linear_technologies[i] = re.sub(\"[^a-zA-Z]\", \" \",converted_linear_technologies[i])\n",
    "        converted_linear_technologies[i] = re.sub('max-width', '', converted_linear_technologies[i])\n",
    "    for i in range(0,len(converted_linear_technologies)):\n",
    "        converted_linear_technologies[i] = \"\".join(c for c in converted_linear_technologies[i] if c not in punc)\n",
    "    converted_linear_technologies = [[feature for feature in clt.split() if feature not in stoplist] \n",
    "         for clt in converted_linear_technologies]\n",
    "    for i in range(0,len(converted_linear_technologies)):\n",
    "        converted_linear_technologies[i] = remove_duplicates(converted_linear_technologies[i])\n",
    "    return converted_linear_technologies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorganize languages feature phrases, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def synthesize_languages(df_col_arg):\n",
    "    converted_languages = df_col_arg\n",
    "    converted_linear_languages = []\n",
    "    for language in converted_languages:\n",
    "        converted_linear_languages.append(str(language).lower())    \n",
    "    for i in range(0,len(converted_linear_languages)):\n",
    "        converted_linear_languages[i] = \"\".join(c for c in converted_linear_languages[i] if c not in punc)\n",
    "    converted_linear_languages = [[feature for feature in cll.split() if feature not in stoplist] \n",
    "         for cll in converted_linear_languages]\n",
    "    for i in range(0,len(converted_linear_languages)):\n",
    "        converted_linear_languages[i] = remove_duplicates(converted_linear_languages[i])    \n",
    "    return converted_linear_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorganize title feature phrases, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def synthesize_title(df_col_arg):\n",
    "    converted_title = df_col_arg\n",
    "    converted_linear_title = []\n",
    "    for title in converted_title:\n",
    "        converted_linear_title.append(str(title).lower())    \n",
    "    for i in range(0,len(converted_linear_title)):\n",
    "        converted_linear_title[i] = re.sub('/', ' ', converted_linear_title[i])\n",
    "        converted_linear_title[i] = re.sub('co-', 'co', converted_linear_title[i])\n",
    "        converted_linear_title[i] = re.sub('vice president', 'vp', converted_linear_title[i])\n",
    "        converted_linear_title[i] = re.sub('chief innovation officer', 'cio', converted_linear_title[i])    \n",
    "        converted_linear_title[i] = re.sub('chief revenue officer', 'cro', converted_linear_title[i])    \n",
    "        converted_linear_title[i] = re.sub('chief executive officer', 'ceo', converted_linear_title[i])    \n",
    "        converted_linear_title[i] = re.sub('chief technology officer', 'cto', converted_linear_title[i])    \n",
    "        converted_linear_title[i] = re.sub('chief creative officer', 'cco', converted_linear_title[i])    \n",
    "        converted_linear_title[i] = re.sub('chief operating officer', 'coo', converted_linear_title[i])    \n",
    "        converted_linear_title[i] = re.sub('chief marketing officer', 'cmo', converted_linear_title[i])    \n",
    "        converted_linear_title[i] = re.sub('chief digital officer', 'cdo', converted_linear_title[i])    \n",
    "        converted_linear_title[i] = re.sub('chief digital transformation officer', 'cdo', converted_linear_title[i])    \n",
    "        converted_linear_title[i] = re.sub('chief growth officer', 'cgo', converted_linear_title[i])    \n",
    "        converted_linear_title[i] = re.sub('chief customer officer', 'ccto', converted_linear_title[i])    \n",
    "        converted_linear_title[i] = re.sub('chief product officer', 'cpo', converted_linear_title[i])    \n",
    "        converted_linear_title[i] = re.sub('chief problem solver', 'cps', converted_linear_title[i])    \n",
    "        converted_linear_title[i] = re.sub('chief people person', '', converted_linear_title[i])    \n",
    "        converted_linear_title[i] = re.sub('chief', '', converted_linear_title[i])    \n",
    "    for i in range(0,len(converted_linear_title)):\n",
    "        converted_linear_title[i] = remove_duplicates(converted_linear_title[i])\n",
    "    for i in range(0,len(converted_linear_title)):\n",
    "        if converted_linear_title[i] is \"MISSING\":\n",
    "            converted_linear_titleloc[i] = \"0\"\n",
    "    return converted_linear_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to identify subscribers on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_conversions_via_indices(df_arg_train): \n",
    "    collect_cosine_similarities = []\n",
    "    conversions = []\n",
    "    for idx in range(df_arg_train.index[0], df_arg_train.index[len(df_arg_train)-1]):\n",
    "        if df_arg_train['converted'].loc[idx] == 1:\n",
    "            conversions.append(idx)\n",
    "    return conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate the continuous result from cosine similiarity to an integer range 0 --> 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binning(input_var):\n",
    "    if input_var <= 0:\n",
    "        return 0\n",
    "    if input_var > 0 and input_var <= .05:\n",
    "        return 1\n",
    "    elif input_var > .05 and input_var <= .10:\n",
    "        return 2\n",
    "    elif input_var > .10 and input_var <= .15:\n",
    "        return 3\n",
    "    elif input_var > .15 and input_var <= .20:\n",
    "        return 4\n",
    "    elif input_var > .20 and input_var <= .25:\n",
    "        return 5\n",
    "    elif input_var > .25 and input_var <= .30:\n",
    "        return 6\n",
    "    elif input_var > .30 and input_var <= .35:\n",
    "        return 7\n",
    "    elif input_var > .35 and input_var <= .40:\n",
    "        return 8\n",
    "    elif input_var > .40 and input_var <= .45:\n",
    "        return 9\n",
    "    elif input_var > .45 and input_var <= .50:\n",
    "        return 10\n",
    "    elif input_var > .50 and input_var <= .55:\n",
    "        return 11\n",
    "    elif input_var > .55 and input_var <= .60:\n",
    "        return 12\n",
    "    elif input_var > .60 and input_var <= .65:\n",
    "        return 13\n",
    "    elif input_var > .65 and input_var <= .70:\n",
    "        return 14\n",
    "    elif input_var > .70 and input_var <= .75:\n",
    "        return 15\n",
    "    elif input_var > .75 and input_var <= .80:\n",
    "        return 16\n",
    "    elif input_var > .80 and input_var <= .85:\n",
    "        return 17\n",
    "    elif input_var > .85 and input_var <= .90:\n",
    "        return 18\n",
    "    elif input_var > .90 and input_var <= .95:\n",
    "        return 19\n",
    "    else:\n",
    "        return 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the binning results into categorical variables for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_into_categorical_vals(list_arg):\n",
    "    categorical = []\n",
    "    for itr_arg in list_arg:\n",
    "        categorical.append(binning(itr_arg))\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill voids in the original dataset to avoid processing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_voids(df_arg):\n",
    "    df_arg['id'].fillna(0, inplace=True)\n",
    "    df_arg['first'].fillna('0', inplace=True)\n",
    "    df_arg['last'].fillna('0', inplace=True)\n",
    "    df_arg['title'].fillna('0', inplace=True)\n",
    "    df_arg['company'].fillna('0', inplace=True)\n",
    "    df_arg['size'].fillna('0', inplace=True)\n",
    "    df_arg['industry'].fillna('0', inplace=True)\n",
    "    df_arg['keywords'].fillna('0', inplace=True)\n",
    "    df_arg['city'].fillna('0', inplace=True)\n",
    "    df_arg['state'].fillna('0', inplace=True)\n",
    "    df_arg['technologies'].fillna('0', inplace=True)\n",
    "    df_arg['languages'].fillna('0', inplace=True)\n",
    "    df_arg['funding'].fillna(0, inplace=True)\n",
    "    df_arg['stage'].fillna('0', inplace=True)\n",
    "    df_arg['amount'].fillna(0, inplace=True)\n",
    "    df_arg['latest'].fillna('0', inplace=True)\n",
    "    df_arg['seo'].fillna('0', inplace=True)\n",
    "    df_arg['converted'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)    \n",
    "\n",
    "def calculate_cosine_vectors(list1_arg, idx_arg, list2_arg): \n",
    "    v1 = []\n",
    "    v2 = []\n",
    "    collect_cosine_similarities = []\n",
    "    convert_transform = []\n",
    "    for idx in range(list1_arg.index[0], list1_arg.index[len(list1_arg)-1]+1):\n",
    "        v1 = text_to_vector(str(list1_arg.loc[idx]))\n",
    "        for idxx in idx_arg:\n",
    "            v2 = text_to_vector(str(list2_arg.loc[idxx]))\n",
    "            collect_cosine_similarities.append(get_cosine(v1, v2))\n",
    "        convert_transform.append(max(collect_cosine_similarities))\n",
    "        collect_cosine_similarities[:] = []\n",
    "    return convert_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Header and categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_vector = ['title', 'keywords', 'industry', 'seo','technologies', 'languages']\n",
    "categorical_variables = ['title', 'company', 'size', 'industry', 'keywords', 'city', 'state', 'technologies', 'languages', 'funding', 'stage', 'latest', 'seo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processes and punctuation tokens for reducing feature phrases into vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punc = ['\\'','!', '.', ':', ',', '\\\\', '&', '|', '+', '-']\n",
    "stoplist = stopwords.words('english')\n",
    "WORD = re.compile(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_merge_input = 'validate.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the start & end of the training data and the start & end of the test data:  \n",
    "train_start is always 0, \n",
    "train_end = #_of_testing_entries - 1,\n",
    "test_start is always train_end + 1,\n",
    "test_end = #_of_all_entries - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_start = 0\n",
    "train_end = 8999\n",
    "\n",
    "test_start = 9000\n",
    "test_end = 12289"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the csv upload then, synthesize and sort the data based on the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = load_csv(csv_merge_input)\n",
    "fill_voids(df)\n",
    "for f_v in feature_vector:\n",
    "    df[f_v] = synthesize_keywords(df[f_v])\n",
    "    for idx in range(0,len(df[f_v])):\n",
    "        df[f_v].loc[idx].sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the subscribers from the training set.  Calculate the cosine similiarities then prepare for feature values into categorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CCTI = identify_conversions_via_indices(df.loc[train_start:train_end])\n",
    "for f_v in feature_vector:\n",
    "    df[f_v] = transform_into_categorical_vals(calculate_cosine_vectors(df[f_v], CCTI, df[f_v].loc[train_start:train_end]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the columns which will not be used as input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop('last', axis=1, inplace=True)\n",
    "df.drop('first', axis=1, inplace=True)\n",
    "df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop out the subscriber results from the dataset and use it as an input to the fit model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df.pop('converted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate the feature values into categorical variables by way of 'get_dummies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for variable in categorical_variables:\n",
    "    dummies = pd.get_dummies(df[variable], prefix=variable)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df.drop([variable], axis=1, inplace=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=500,oob_score=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the data to the model (rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(df.loc[train_start:train_end], y[train_start:train_end+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the solution through 'predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "solution = model.predict(df.loc[test_start:test_end])\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.12690456e-02,   2.28415487e-06,   9.88153637e-09, ...,\n",
       "         3.51849286e-08,   1.26953750e-05,   6.36639356e-02])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-aggregate the categorical variables into feature variables and aggregate their impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph_feature_importance(model, feature_names, autoscale=True, headroom=.09, width=10, summarized_columns=None):\n",
    "    if autoscale:\n",
    "        x_scale = model.feature_importances_.max()+2*headroom\n",
    "    else:\n",
    "        x_scale = 1\n",
    "    feature_dict = dict(zip(feature_names, model.feature_importances_))\n",
    "    if summarized_columns:\n",
    "        for col_name in summarized_columns:\n",
    "            sum_value = sum(x for i, x in feature_dict.iteritems() if col_name in i)\n",
    "            keys_to_remove = [i for i in feature_dict.keys() if col_name in i]\n",
    "            for i in keys_to_remove:\n",
    "                feature_dict.pop(i)    \n",
    "            feature_dict[col_name] = sum_value\n",
    "    results = pd.Series(feature_dict.values(), index=feature_dict.keys())\n",
    "    results.sort(axis=1)\n",
    "    results.plot(kind=\"barh\", figsize=(width, len(results)/4),xlim=(0,x_scale), fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plot the results from the aggregated feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_feature_importance(model, df.columns, summarized_columns=categorical_variables);\n",
    "plot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
